{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a18578f-9284-4629-aa6b-f9347e7830e9",
   "metadata": {
    "language": "sql",
    "name": "cell6"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE DATABASE ICEBERG_LAKE;\n",
    "CREATE OR REPLACE SCHEMA ICEBERG_LAKE.DEMO;\n",
    "USE SCHEMA ICEBERG_LAKE.DEMO;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a51b4ef-3351-4b67-ab48-3d32095d29a8",
   "metadata": {
    "collapsed": false,
    "name": "cell25"
   },
   "source": [
    "### Snowflake has deep iceberg integration. You can add an external catalog and its Iceberg tables automatically using catalog linked database with read/write. So lets create the Glue REST catalog integration with Snowflake Db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89e17ee-052f-4ad9-9b23-73aa2c654c94",
   "metadata": {
    "language": "sql",
    "name": "cell26"
   },
   "outputs": [],
   "source": [
    "-- Create and integrate Snowflake with your object storage as External volume\n",
    "\n",
    "CREATE OR REPLACE EXTERNAL VOLUME extvol_iceberg_demo\n",
    "STORAGE_LOCATIONS =\n",
    "      (\n",
    "         (\n",
    "            NAME = 'Iceberg-Table-Demo'\n",
    "            STORAGE_PROVIDER = 'S3'\n",
    "            STORAGE_BASE_URL = 's3://<Your dataBucket name>/'\n",
    "            STORAGE_AWS_ROLE_ARN = '<AWS ROLE ARN>'\n",
    "            STORAGE_AWS_EXTERNAL_ID = '<Any secret word you want>'\n",
    "         )\n",
    "      );\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f468c903-e58d-4df2-811c-cd0e55aaeb00",
   "metadata": {
    "language": "sql",
    "name": "cell27"
   },
   "outputs": [],
   "source": [
    "desc EXTERNAL VOLUME extvol_iceberg_demo;\n",
    "\n",
    "SELECT b.KEY, b.VALUE \n",
    "FROM \n",
    "    TABLE(RESULT_SCAN(LAST_QUERY_ID())) a, TABLE(FLATTEN(INPUT => PARSE_JSON(a.\"property_value\"))) b \n",
    "where \n",
    "    a.\"parent_property\" = 'STORAGE_LOCATIONS'\n",
    "and a.\"property\" = 'STORAGE_LOCATION_1'\n",
    "and (b.KEY='STORAGE_AWS_EXTERNAL_ID' OR b.KEY ='STORAGE_AWS_IAM_USER_ARN');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af84e5f6-0225-4542-a3e6-9429baa03a61",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "cell10"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "-- Create Catalog integration for Iceberg read/write support with external REST based Catalog glue_rest_cat_int\n",
    "\n",
    "CREATE OR REPLACE CATALOG INTEGRATION glue_rest_cat_int_demo\n",
    "  CATALOG_SOURCE = ICEBERG_REST\n",
    "  TABLE_FORMAT = ICEBERG\n",
    "  CATALOG_NAMESPACE = '<Glue database Name>'\n",
    "  REST_CONFIG = (\n",
    "    CATALOG_URI = 'https://glue.<AWS REGION>.amazonaws.com/iceberg'\n",
    "    CATALOG_API_TYPE = AWS_GLUE\n",
    "    CATALOG_NAME = '<AWS ACCOUNT NUMBER>'\n",
    "  )\n",
    "  REST_AUTHENTICATION = (\n",
    "    TYPE = SIGV4\n",
    "    SIGV4_IAM_ROLE = '<AWS ROLE ARN>'\n",
    "    SIGV4_SIGNING_REGION = '<AWS REGION like US-WEST-2>'\n",
    "    SIGV4_EXTERNAL_ID = '<External ID used in cloud formation>'\n",
    "  )\n",
    "  ENABLED = TRUE;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0018908-4ad9-4211-a1ce-2a07a33c5a32",
   "metadata": {
    "language": "sql",
    "name": "cell29"
   },
   "outputs": [],
   "source": [
    "desc catalog integration glue_rest_cat_int_demo;\n",
    "\n",
    "SELECT \"property\",\"property_value\" \n",
    "FROM \n",
    "    TABLE(RESULT_SCAN(LAST_QUERY_ID())) \n",
    "WHERE\n",
    "    \"property\" IN ('API_AWS_IAM_USER_ARN','API_AWS_EXTERNAL_ID');\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811abb8b-5b68-4366-ae6d-e245564f303b",
   "metadata": {
    "language": "sql",
    "name": "cell28"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "CREATE DATABASE IF NOT EXISTS glue_lake_int_db\n",
    "  LINKED_CATALOG = (\n",
    "    CATALOG = 'glue_rest_cat_int_demo',\n",
    "    ALLOWED_NAMESPACES = ('<Glue database name>'),\n",
    "    NAMESPACE_MODE = FLATTEN_NESTED_NAMESPACE,\n",
    "    NAMESPACE_FLATTEN_DELIMITER = '-'\n",
    "    SYNC_INTERVAL_SECONDS = 60\n",
    "  )\n",
    "  EXTERNAL_VOLUME = 'extvol_iceberg_demo';\n",
    "\n",
    "--SELECT SYSTEM$CATALOG_LINK_STATUS('ext_lake_glue_db');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110a8f59-134b-41a2-baa9-5a6ae9c6fcb3",
   "metadata": {
    "language": "sql",
    "name": "cell30"
   },
   "outputs": [],
   "source": [
    "SELECT SYSTEM$CATALOG_LINK_STATUS('glue_lake_int_db');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43c1ef6-e598-42a3-944d-3d55a1c649de",
   "metadata": {
    "language": "sql",
    "name": "cell20"
   },
   "outputs": [],
   "source": [
    "use database glue_lake_int_db;\n",
    "use schema \"<glue database name>\";\n",
    "show schemas;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef45c39f-9b7f-4398-b5ba-091fcae6d9d3",
   "metadata": {
    "collapsed": false,
    "name": "cell4"
   },
   "source": [
    "## Copy and Paste your Spark Code and it just works !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5e5799-87fd-4226-b82e-3e06dbe65688",
   "metadata": {
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": [
    "# Catalog configuration\n",
    "catalog = \"GLUE_LAKE_INT_DB\"\n",
    "database = \"<Glue database Name>\"\n",
    "source_sales_table = \"raw_sales_data\"  \n",
    "report_table = \"top_10_products_report_snow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3ad593-d3f5-4b54-a77a-b5d0208fcdde",
   "metadata": {
    "language": "sql",
    "name": "cell32"
   },
   "outputs": [],
   "source": [
    "show tables;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b846bd7-732f-445c-a5ab-ba1174dd3a1e",
   "metadata": {
    "collapsed": false,
    "name": "cell5"
   },
   "source": [
    "### Update spark context and use Snowpark Connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11faa20-5024-41a9-b904-da2fc0ecae81",
   "metadata": {
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": [
    "from snowflake import snowpark_connect\n",
    "import traceback\n",
    "\n",
    "spark = snowpark_connect.server.init_spark_session()\n",
    "\n",
    "# Create Spark Session and write normal Spark code.\n",
    "spark = snowpark_connect.get_session()\n",
    "\n",
    "# Glue database objects are auto lowercase, we will use double qoutes to identify the tables and column\n",
    "spark.conf.set(\"snowpark.connect.sql.identifiers.auto-uppercase\", \"none\")\n",
    "#spark.conf.set(\"snowpark.connect.iceberg.external_volume\",\"extvol_iceberg_demo1\")\n",
    "\n",
    "# Use lowercase table name (most common in Glue)\n",
    "query = f\"select customer_name, purchases from {catalog}.{database}.{source_sales_table}\"\n",
    "spark.sql(query).show(5, truncate = False)\n",
    "#spark.sql(\"SHOW TABLES\").show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8a18d5-2255-4384-9e75-aec88571d437",
   "metadata": {
    "collapsed": false,
    "name": "cell2"
   },
   "source": [
    "# Lets build a sales report for Top 10 Highest Selling Products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf656d7-d631-4ae8-ae74-bd0c418d6697",
   "metadata": {
    "collapsed": false,
    "name": "cell12"
   },
   "source": [
    "#### Step 1: Read from existing sales Iceberg table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, sum as spark_sum, count, avg, max as spark_max, \n",
    "    min as spark_min, desc, asc, round as spark_round,\n",
    "    to_date, when, isnan, isnull, current_date, countDistinct\n",
    ")\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType\n",
    "\n",
    "job_name = \"AWS Glue Top 10 Product Sales Report\"\n",
    "\n",
    "print(\"=== Starting Top 10 Highest Selling Products Report from Iceberg Table ===\")\n",
    "\n",
    "# Step 1: Read from existing Iceberg table (created from gzipped data)\n",
    "print(f\"\\n1. Reading sales data from Iceberg table: {catalog}.{database}.{source_sales_table}\")\n",
    "try:\n",
    "    # Read from the existing Iceberg table created by glue_script_gzip.py\n",
    "    sales_iceberg_df = spark.table(f\"{catalog}.{database}.{source_sales_table}\")\n",
    "    record_count = sales_iceberg_df.count()\n",
    "    print(f\"Sales data loaded successfully from Iceberg table. Total records: {record_count:,}\")\n",
    "    \n",
    "    print(\"Iceberg table schema:\")\n",
    "    sales_iceberg_df.printSchema()    \n",
    "    \n",
    "    print(\"Sample data from Iceberg table:\")\n",
    "    sales_iceberg_df.show(3, truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error reading from Iceberg table: {e}\")\n",
    "    print(\"Make sure the Iceberg table exists and has been populated by running glue_script_gzip.py first\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e3d419-f18e-48c9-bc6a-af544adf128f",
   "metadata": {
    "collapsed": false,
    "name": "cell16"
   },
   "source": [
    "#### Step 2: Extract and flatten the nested purchase data from Iceberg table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42fd5ae-0de3-469f-8568-19625b541cec",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Extract and flatten the nested purchase data from Iceberg table\n",
    "print(\"\\n2. Extracting and flattening purchase data from Iceberg table...\")\n",
    "try:\n",
    "    # Extract nested purchase data from the struct column\n",
    "    flattened_sales = sales_iceberg_df.select(\n",
    "        col(\"customer_id\").alias(\"customer_id\"),\n",
    "        col(\"customer_name\").alias(\"customer_name\"),\n",
    "        col(\"purchases.prodid\").alias(\"product_id\"),\n",
    "        col(\"purchases.purchase_amount\").alias(\"purchase_amount\"),\n",
    "        col(\"purchases.quantity\").alias(\"quantity\"),\n",
    "        col(\"purchases.purchase_date\").alias(\"purchase_date\")\n",
    "    )\n",
    "    \n",
    "    # Remove records with null or invalid product IDs\n",
    "    clean_sales = flattened_sales.filter(\n",
    "        col(\"product_id\").isNotNull() & \n",
    "        (col(\"product_id\") > 0) & \n",
    "        col(\"purchase_amount\").isNotNull() & \n",
    "        (col(\"purchase_amount\") > 0) &\n",
    "        col(\"quantity\").isNotNull() & \n",
    "        (col(\"quantity\") > 0)\n",
    "    )\n",
    "    \n",
    "    print(f\"Cleaned sales data. Records after cleaning: {clean_sales.count():,}\")\n",
    "    print(\"Sample cleaned data:\")\n",
    "    clean_sales.show(5, truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error flattening Iceberg table data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ef6ab4-17b8-4a99-8a24-f199c937a768",
   "metadata": {
    "collapsed": false,
    "name": "cell17"
   },
   "source": [
    "#### Step 3: Aggregate sales data by product ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6948ee01-95d9-4f1a-b44c-f2f9eb87e338",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": [
    "# Step 3: Aggregate sales data by product ID\n",
    "print(\"\\n3. Aggregating sales data by product ID...\")\n",
    "try:\n",
    "    product_sales_summary = clean_sales.groupBy(\"product_id\").agg(\n",
    "        spark_sum(\"purchase_amount\").alias(\"total_sales_amount\"),\n",
    "        spark_sum(\"quantity\").alias(\"total_quantity_sold\"),\n",
    "        count(\"*\").alias(\"total_transactions\"),\n",
    "        avg(\"purchase_amount\").alias(\"avg_transaction_amount\"),\n",
    "        spark_max(\"purchase_amount\").alias(\"max_transaction_amount\"),\n",
    "        spark_min(\"purchase_amount\").alias(\"min_transaction_amount\"),\n",
    "        avg(\"quantity\").alias(\"avg_quantity_per_transaction\"),\n",
    "        spark_max(\"quantity\").alias(\"max_quantity_per_transaction\"),\n",
    "        countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
    "        spark_min(\"purchase_date\").alias(\"first_sale_date\"),\n",
    "        spark_max(\"purchase_date\").alias(\"last_sale_date\")\n",
    "    ).withColumn(\n",
    "        \"avg_price_per_unit\", \n",
    "        spark_round(col(\"total_sales_amount\") / col(\"total_quantity_sold\"), 2)\n",
    "    ).withColumn(\n",
    "        \"avg_transaction_amount\", \n",
    "        spark_round(col(\"avg_transaction_amount\"), 2)\n",
    "    ).withColumn(\n",
    "        \"avg_quantity_per_transaction\", \n",
    "        spark_round(col(\"avg_quantity_per_transaction\"), 2)\n",
    "    )\n",
    "    \n",
    "    print(f\"Product aggregation completed. Unique products: {product_sales_summary.count():,}\")\n",
    "    print(\"Top 5 products by sales amount:\")\n",
    "    product_sales_summary.orderBy(desc(\"total_sales_amount\")).show(5, truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error aggregating sales data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b83450a-8477-4859-bbd5-fdacba3fd78d",
   "metadata": {
    "collapsed": false,
    "name": "cell18"
   },
   "source": [
    "#### Step 4: Generate Top 10 Highest Selling Products Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1bbe6b-8f8e-4e2d-99e4-00e4fe854b65",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Generate Top 10 Highest Selling Products Report\n",
    "print(\"\\n4. Generating Top 10 Highest Selling Products Report...\")\n",
    "try:\n",
    "    # Rank by total sales amount\n",
    "    top_10_products = product_sales_summary.orderBy(desc(\"total_sales_amount\")).limit(10)\n",
    "    \n",
    "    print(\"\\n=== TOP 10 HIGHEST SELLING PRODUCTS BY SALES AMOUNT ===\")\n",
    "    top_10_products.show(10, truncate=False)\n",
    "    \n",
    "    # Also show top 10 by quantity for comparison\n",
    "    top_10_by_quantity = product_sales_summary.orderBy(desc(\"total_quantity_sold\")).limit(10)\n",
    "    \n",
    "    print(\"\\n=== TOP 10 HIGHEST SELLING PRODUCTS BY QUANTITY SOLD ===\")\n",
    "    top_10_by_quantity.show(10, truncate=False)\n",
    "    \n",
    "    # Top 10 by customer reach\n",
    "    top_10_by_reach = product_sales_summary.orderBy(desc(\"unique_customers\")).limit(10)\n",
    "    \n",
    "    print(\"\\n=== TOP 10 PRODUCTS BY CUSTOMER REACH ===\")\n",
    "    top_10_by_reach.show(10, truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error generating top products report: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0122136-6b2d-4b00-97e5-a3fcd494b9e1",
   "metadata": {
    "collapsed": false,
    "name": "cell21"
   },
   "source": [
    "#### Step 5: Create detailed report with rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27047a30-0c71-4967-b85e-8dae07b7c36f",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Step 5: Create detailed report with rankings\n",
    "print(\"\\n5. Creating detailed report with rankings...\")\n",
    "try:\n",
    "    from pyspark.sql.window import Window\n",
    "    from pyspark.sql.functions import row_number, rank, dense_rank, lit\n",
    "    \n",
    "    # Create window specification for ranking\n",
    "    sales_window = Window.partitionBy(lit(1)).orderBy(desc(\"total_sales_amount\"))\n",
    "    quantity_window = Window.partitionBy(lit(1)).orderBy(desc(\"total_quantity_sold\"))  \n",
    "    customer_window = Window.partitionBy(lit(1)).orderBy(desc(\"unique_customers\"))\n",
    "    \n",
    "    detailed_report = product_sales_summary.withColumn(\n",
    "        \"sales_amount_rank\", row_number().over(sales_window)\n",
    "    ).withColumn(\n",
    "        \"quantity_rank\", row_number().over(quantity_window)\n",
    "    ).withColumn(\n",
    "        \"customer_reach_rank\", row_number().over(customer_window)\n",
    "    ).filter(col(\"sales_amount_rank\") <= 10).orderBy(\"sales_amount_rank\")\n",
    "    \n",
    "    print(\"\\n=== DETAILED TOP 10 PRODUCTS REPORT (1M Records Analysis) ===\")\n",
    "    detailed_report.select(\n",
    "        \"sales_amount_rank\",\n",
    "        \"product_id\",\n",
    "        \"total_sales_amount\",\n",
    "        \"total_quantity_sold\",\n",
    "        \"unique_customers\",\n",
    "        \"total_transactions\",\n",
    "        \"avg_price_per_unit\",\n",
    "        \"avg_transaction_amount\",\n",
    "        \"first_sale_date\",\n",
    "        \"last_sale_date\"\n",
    "    ).show(10, truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating detailed report: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfdd786-a550-4f61-994d-7fa66c158cfd",
   "metadata": {
    "collapsed": false,
    "name": "cell23"
   },
   "source": [
    "#### Step 6: Save report to Iceberg table created for snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93985ffe-4fe4-4785-ba5b-db38306c9a90",
   "metadata": {
    "language": "python",
    "name": "cell15"
   },
   "outputs": [],
   "source": [
    "print(\"\\n6. Saving report to Iceberg table...\")\n",
    "try:\n",
    "\n",
    "   \n",
    "    # Add report metadata and save\n",
    "    from pyspark.sql.functions import current_date, lit\n",
    "    \n",
    "    final_report = detailed_report.withColumn(\"report_date\", current_date()) \\\n",
    "                                 .withColumn(\"source_table\", lit(source_sales_table)) \\\n",
    "                                 .withColumn(\"job_name\", lit(job_name))   \n",
    "   \n",
    "    # Get the target table's column order\n",
    "    target_df = spark.read.table(f\"{catalog}.{database}.{report_table}\")\n",
    "\n",
    "    spark.sql(f\"TRUNCATE TABLE {catalog}.{database}.{report_table}\")\n",
    "    final_report.select(*target_df.columns).write.insertInto(f\"{catalog}.{database}.{report_table}\")    \n",
    "    \n",
    "    print(f\"Report saved successfully to {catalog}.{database}.{report_table}\")\n",
    "    target_df.show(10, truncate=False)\n",
    "except Exception as e:\n",
    "    print(f\"Error saving report to Iceberg: {e}\")\n",
    "\n",
    "\n",
    "print(f\" Source: Iceberg table {catalog}.{database}.{source_sales_table}\")\n",
    "print(f\" Report saved to: {catalog}.{database}.{report_table}\")\n",
    "print(f\" Top 10 products analysis completed\")\n",
    "\n",
    "print(f\"\\n Sales report generation completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1a5829-89b3-401d-bd5b-ac11bfe7df7b",
   "metadata": {
    "collapsed": false,
    "name": "cell1"
   },
   "source": [
    "### Compare this to the GLUE Spark job results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ce53fb-d965-4809-9268-3e261d616d04",
   "metadata": {
    "language": "python",
    "name": "cell35"
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"select * from {catalog}.{database}.{report_table} \").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb66ed8-f296-4c7b-9b42-921f4e7ce8d5",
   "metadata": {
    "language": "python",
    "name": "cell22"
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e896b5c5-26ed-49ad-b854-c2835421bf1e",
   "metadata": {
    "collapsed": false,
    "name": "cell19"
   },
   "source": [
    "### Migrating Spark Connect jobs for the dataframe API is now very easy. You can keep your existing code. The catalog linked database makes it super simple to integrate the existing Iceberg catalog with write enable capabilities, you can keep your data where it is. This enahance interoperability opens up new venues for Lakehouse architecture."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "authorEmail": "Parag.Jain@snowflake.com",
   "authorId": "415489952713",
   "authorName": "PJAIN",
   "lastEditTime": 1764097543331,
   "notebookId": "gzglsajtjmjqekf6nxby",
   "sessionId": "140168a9-290e-4aa1-9f8e-e978bf69e383"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
